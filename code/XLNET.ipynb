{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1NJLiM9nXE9"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import pickle\n",
        "import nltk\n",
        "import re\n",
        "import os, json\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
        "import math\n",
        "import torch\n",
        "# import torch.nn as nn\n",
        "from torch.nn import BCEWithLogitsLoss, NLLLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
        "# from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
        "from transformers import AdamW, XLNetTokenizer, XLNetModel, TFXLNetModel, XLNetLMHeadModel, XLNetConfig, XLNetForSequenceClassification\n",
        "#from keras.utils.data_utils import pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "import matplotlib.pyplot as plt\n",
        "import unicodedata\n",
        "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix,roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score,precision_recall_curve\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from numpy import trapz\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score,balanced_accuracy_score\n",
        "from sklearn.metrics import average_precision_score,cohen_kappa_score\n",
        "from sklearn.model_selection import KFold\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import plotly\n",
        "import seaborn as sns\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from collections import Counter  \n",
        "import time\n",
        "import datetime\n",
        "plt.style.use('seaborn')\n",
        "#from transformers import TFXLNetModel, XLNetTokenizer\n",
        "from transformers import XLNetTokenizer, XLNetModel\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import itertools\n",
        "import statistics\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "    \n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+' \n",
        " \n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')    \n",
        "def preprocess_sentence_english(w):\n",
        "\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  \n",
        "  w = re.sub(r\"@[A-Za-z0-9]+\", ' ', w)\n",
        "  w = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', w)\n",
        "  w = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', w)\n",
        "  w = re.sub('\\t', ' ',  w)\n",
        "  w = re.sub(r\" +\", ' ', w)\n",
        "    \n",
        " \n",
        "  #chars (optional) - a string specifying the set of characters to be removed.\n",
        "  #If the chars argument is not provided, all leading and trailing whitespaces are removed from the string.\n",
        "  w = w.rstrip().strip()\n",
        "\n",
        " \n",
        "  # Fix misspelled words\n",
        "  w = ''.join(''.join(s)[:2] for _, s in itertools.groupby(w))# checking that each character should occur not more than 2 times in every word\n",
        "\n",
        "  # Tokenizing ,change cases & join together to remove unneccessary white spaces\n",
        "  w = tok.tokenize(w)\n",
        "  w = (\" \".join(w)).strip()\n",
        "  return w\n",
        "\n",
        "def tokenize_inputs(text_list, tokenizer, num_embeddings=120):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text input into ids. Appends the appropriate special\n",
        "    characters to the end of the text to denote end of sentence. Truncate or pad\n",
        "    the appropriate sequence length.\n",
        "    \"\"\"\n",
        "    # tokenize the text, then truncate sequence to the desired length minus 2 for\n",
        "    # the 2 special characters\n",
        "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n",
        "    # convert tokenized text into numeric ids for the appropriate LM\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    # append special token \"<s>\" and </s> to end of sentence\n",
        "    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n",
        "    # pad sequences\n",
        "    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    return input_ids\n",
        "    \n",
        "    \n",
        "def create_attn_masks(input_ids):\n",
        "    \"\"\"\n",
        "    Create attention masks to tell model whether attention should be applied to\n",
        "    the input id tokens. Do not want to perform attention on padding tokens.\n",
        "    \"\"\"\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return attention_masks\n",
        "    \n",
        "    \n",
        "class XLNetForMultiLabelSequenceClassification(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, num_labels=2):\n",
        "    super(XLNetForMultiLabelSequenceClassification, self).__init__()\n",
        "    self.num_labels = num_labels\n",
        "    self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n",
        "    self.classifier = torch.nn.Linear(768, num_labels)\n",
        "\n",
        "    torch.nn.init.xavier_normal_(self.classifier.weight)\n",
        "\n",
        "  def forward(self, input_ids, token_type_ids=None,\\\n",
        "              attention_mask=None, labels=None):\n",
        "       \n",
        "    # last hidden layer\n",
        "    last_hidden_state = self.xlnet(input_ids=input_ids,\\\n",
        "                                   attention_mask=attention_mask,\\\n",
        "                                   token_type_ids=token_type_ids\n",
        "                                  )\n",
        "    # pool the outputs into a mean vector\n",
        "    mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n",
        "    logits = self.classifier(mean_last_hidden_state)\n",
        "   \n",
        "    logits = logits[:, 1] - logits[:, 0]\n",
        "    \n",
        "    \n",
        "    if labels is not None:\n",
        "\n",
        "      loss = BCEWithLogitsLoss()(logits, labels.float())\n",
        "\n",
        "      return loss\n",
        "    else:\n",
        "      return logits\n",
        "    \n",
        "  def freeze_xlnet_decoder(self):\n",
        "    \"\"\"\n",
        "    Freeze XLNet weight parameters. They will not be updated during training.\n",
        "    \"\"\"\n",
        "    for param in self.xlnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "  def unfreeze_xlnet_decoder(self):\n",
        "    \"\"\"\n",
        "    Unfreeze XLNet weight parameters. They will be updated during training.\n",
        "    \"\"\"\n",
        "    for param in self.xlnet.parameters():\n",
        "      param.requires_grad = True\n",
        "  def pool_hidden_state(self, last_hidden_state):\n",
        "    \"\"\"\n",
        "    Pool the output vectors into a single mean vector \n",
        "    \"\"\"\n",
        "    last_hidden_state = last_hidden_state[0]\n",
        "    mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n",
        "    return mean_last_hidden_state    \n",
        "  \n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "def train_def(model, num_epochs,\\\n",
        "          optimizer,\\\n",
        "          train_dataloader, valid_dataloader,\\\n",
        "          model_save_path,\\\n",
        "          train_loss_set=[], valid_loss_set = [],\\\n",
        "          lowest_eval_loss=None, start_epoch=0,\\\n",
        "          device=\"cpu\"\n",
        "          ):\n",
        "  \"\"\"\n",
        "  Train the model and save the model with the lowest validation loss\n",
        "  \"\"\"\n",
        "  # We'll store a number of quantities such as training and validation loss, \n",
        "  # validation accuracy, and timings.\n",
        "  training_stats = []\n",
        "  # Measure the total training time for the whole run.\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  # trange is a tqdm wrapper around the python range function\n",
        "  for i in trange(num_epochs, desc=\"Epoch\"):\n",
        "    # if continue training from saved model\n",
        "    actual_epoch = start_epoch + i\n",
        "   \n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set. \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(actual_epoch, num_epochs))\n",
        "    print('Training...')\n",
        "    \n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    \n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    num_train_samples = 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0: \n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "            \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        loss = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # store train loss\n",
        "        tr_loss += loss.item()\n",
        "        num_train_samples += b_labels.size(0)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Gradient Clipping \n",
        "        torch.nn.utils.clip_grad_norm(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        #scheduler.step()\n",
        "\n",
        "    # Update tracking variables\n",
        "    epoch_train_loss = tr_loss/num_train_samples\n",
        "    train_loss_set.append(epoch_train_loss)\n",
        "\n",
        "#     print(\"Train loss: {}\".format(epoch_train_loss))\n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(epoch_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "    \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "    \n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    \n",
        "    # Put model in evaluation mode to evaluate loss on the validation set\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss = 0\n",
        "    num_eval_samples = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in valid_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Telling the model not to compute or store gradients,\n",
        "        # saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate validation loss\n",
        "            loss = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "            # store valid loss\n",
        "            eval_loss += loss.item()\n",
        "            num_eval_samples += b_labels.size(0)\n",
        "\n",
        "    epoch_eval_loss = eval_loss/num_eval_samples\n",
        "    valid_loss_set.append(epoch_eval_loss)\n",
        "\n",
        "#     print(\"Valid loss: {}\".format(epoch_eval_loss))\n",
        "    \n",
        "    # Report the final accuracy for this validation run.\n",
        "#     avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "#     print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "#     avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(epoch_eval_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': actual_epoch,\n",
        "            'Training Loss': epoch_train_loss,\n",
        "            'Valid. Loss': epoch_eval_loss,\n",
        "#             'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    \n",
        "    if lowest_eval_loss == None:\n",
        "      lowest_eval_loss = epoch_eval_loss\n",
        "      # save model\n",
        "      save_model(model, model_save_path, actual_epoch,\\\n",
        "                 lowest_eval_loss, train_loss_set, valid_loss_set)\n",
        "    else:\n",
        "      if epoch_eval_loss < lowest_eval_loss:\n",
        "        lowest_eval_loss = epoch_eval_loss\n",
        "        # save model\n",
        "        save_model(model, model_save_path, actual_epoch,\\\n",
        "                   lowest_eval_loss, train_loss_set, valid_loss_set)\n",
        "  \n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "  return model, train_loss_set, valid_loss_set, training_stats  \n",
        "\n",
        "\n",
        "\n",
        "# function to save and load the model form a specific epoch\n",
        "def save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist):\n",
        "  \"\"\"\n",
        "  Save the model to the path directory provided\n",
        "  \"\"\"\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model\n",
        "  checkpoint = {'epochs': epochs, \\\n",
        "                'lowest_eval_loss': lowest_eval_loss,\\\n",
        "                'state_dict': model_to_save.state_dict(),\\\n",
        "                'train_loss_hist': train_loss_hist,\\\n",
        "                'valid_loss_hist': valid_loss_hist,\\\n",
        "                'optimizer_state_dict': optimizer.state_dict()\n",
        "               }\n",
        "               \n",
        "  torch.save(checkpoint, save_path)\n",
        "  print(\"Saving model at epoch {} with validation loss of {}\".format(epochs,\\\n",
        "                                                                     lowest_eval_loss))\n",
        "  return\n",
        "def load_model(save_path):\n",
        "  \"\"\"\n",
        "  Load the model from the path directory provided\n",
        "  \"\"\"\n",
        "  checkpoint = torch.load(save_path)\n",
        "  model_state_dict = checkpoint['state_dict']\n",
        "  model = XLNetForMultiLabelSequenceClassification(num_labels=model_state_dict[\"classifier.weight\"].size()[0])\n",
        "  model.load_state_dict(model_state_dict)\n",
        "\n",
        "  epochs = checkpoint[\"epochs\"]\n",
        "  lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\n",
        "  train_loss_hist = checkpoint[\"train_loss_hist\"]\n",
        "  valid_loss_hist = checkpoint[\"valid_loss_hist\"]\n",
        "  \n",
        "  return model, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist \n",
        "\n",
        "def generate_predictions(model, df, device=\"cpu\", batch_size=16):\n",
        "  num_iter = math.ceil(df.shape[0]/batch_size)\n",
        "  \n",
        "  pred_probs = []\n",
        "\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  \n",
        "  for i in range(num_iter):\n",
        "    df_subset = df.iloc[i*batch_size:(i+1)*batch_size,:]\n",
        "    X = df_subset[\"features\"].values.tolist()\n",
        "    masks = df_subset[\"masks\"].values.tolist()\n",
        "    X = torch.tensor(X)\n",
        "    masks = torch.tensor(masks, dtype=torch.long)\n",
        "    X = X.to(device)\n",
        "    masks = masks.to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids=X, attention_mask=masks)\n",
        "      logits = logits.sigmoid().detach().cpu().numpy()\n",
        "#       pred_probs = np.vstack([pred_probs, logits])\n",
        "      pred_probs.extend(logits.tolist())\n",
        "        \n",
        "  return pred_probs\n",
        "\n",
        "\n",
        "def Evaluate(labels, predictions):\n",
        "    avg = sum(predictions)/len(predictions)\n",
        "    p=avg\n",
        "    CM= confusion_matrix(labels, predictions > p)\n",
        "    TN = CM[0][0]\n",
        "    FN = CM[1][0]\n",
        "    TP = CM[1][1]\n",
        "    FP = CM[0][1]\n",
        "    print(' (True Negatives): {}'.format(TN))\n",
        "    print(' (False Negatives):  {}'.format(FN))\n",
        "    print(' (True Positives): {}'.format(TP))\n",
        "    print('(False Positives):{}'.format(FP))\n",
        "    print('Total positive : ', np.sum(CM[1]))\n",
        "    auc = roc_auc_score(labels, predictions)\n",
        "    prec=precision_score(labels, predictions>p)\n",
        "    rec=recall_score(labels, predictions>p)\n",
        "     # calculate F1 score\n",
        "    f1 = f1_score(labels, predictions>p)\n",
        "    print('auc :{}'.format(auc))\n",
        "    print('precision :{}'.format(prec))\n",
        "    print('recall :{}'.format(rec))\n",
        "    print('f1 :{}'.format(f1))\n",
        "    # Compute Precision-Recall and plot curve\n",
        "    precision, recall, thresholds = precision_recall_curve(labels, predictions >p)\n",
        "    #use the trapezoidal rule to calculate the area under the precion-recall curve\n",
        "    area =  trapz(recall, precision)\n",
        "   \n",
        "    #area =  simps(recall, precision)\n",
        "    print(\"Area Under Precision Recall  Curve(AP): %0.4f\" % area)   #should be same as AP? \n",
        "    average_precision = average_precision_score(labels, predictions>p)\n",
        "    print(\"average precision: %0.4f\" % average_precision)\n",
        "    kappa = cohen_kappa_score(labels, predictions>p)\n",
        "    print('kappa :{}'.format(kappa))\n",
        "    balanced_accuracy = balanced_accuracy_score(labels, predictions>p)\n",
        "    print('balanced_accuracy :{}'.format(balanced_accuracy))\n",
        "    return prec, rec, f1\n",
        "    \n",
        "def freeze(model, depth):\n",
        "    if depth:\n",
        "        model.xlnet.word_embedding.requires_grad_(False)\n",
        "        for d in range(depth):\n",
        "              model.xlnet.layer[d].rel_attn.layer_norm.requires_grad_(False)\n",
        "              model.xlnet.layer[d].rel_attn.dropout.p = 0.\n",
        "              model.xlnet.layer[d].ff.layer_norm.requires_grad_(False)\n",
        "              model.xlnet.layer[d].ff.layer_1.requires_grad_(False)\n",
        "              model.xlnet.layer[d].ff.layer_2.requires_grad_(False)\n",
        "              model.xlnet.layer[d].ff.dropout.p = 0.\n",
        "              model.xlnet.layer[d].dropout.p = 0.\n",
        "\n",
        "def freeze_params(model):\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    k_folds = 10\n",
        "    max_grad_norm = 1.0\n",
        "    batch_size = 16\n",
        "    num_epochs = 10\n",
        "    MAX_LEN = 250\n",
        "    #Data Augmentation\n",
        "    train = pd.read_csv(r'../input/stress-analysis-in-social-media/dreaddit-train.csv')\n",
        "    train_x= train.text.tolist()\n",
        "    train_y= train.label.tolist()\n",
        "    X = train_x \n",
        "    y = train_y \n",
        "    data = list(zip(X, y))\n",
        "    random.shuffle(data)\n",
        "    data = pd.DataFrame(data, columns=['text','tag'])\n",
        "    X = data.text.tolist()\n",
        "    y = data.tag.tolist()\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(42)\n",
        "     # Define the K-fold Cross Validator\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "    P = []\n",
        "    R = []\n",
        "    F1 = []\n",
        "    for train_index, test_index in kfold.split(X, y):\n",
        "      X_train, X_test = X[train_index], X[test_index]\n",
        "      y_train, y_test = y[train_index], y[test_index]\n",
        "      data_train = list(zip(X_train,y_train))\n",
        "      data_test = list(zip(X_test,y_test))\n",
        "      train = pd.DataFrame(data_train, columns=['text','tag'])\n",
        "      test = pd.DataFrame(data_test, columns=['text','tag'])\n",
        "      print('Number of training sentences: {:,}\\n'.format(train.shape[0]))\n",
        "      print('Number of testing sentences: {:,}\\n'.format(test.shape[0]))\n",
        "      \n",
        "            \n",
        "      tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "      model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')\n",
        "      \n",
        "      # cleaning tweets\n",
        "      train['text_cleaned'] = list(map(lambda x: preprocess_sentence_english(x),train['text']) )\n",
        "      test['text_cleaned'] = list(map(lambda x: preprocess_sentence_english(x),test['text']) )\n",
        "      \n",
        "      sentences = train.text_cleaned.values\n",
        "      labels = train.tag.values\n",
        "            \n",
        "      # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "      \n",
        "      input_ids = tokenize_inputs(sentences, tokenizer, num_embeddings=MAX_LEN)\n",
        "      attention_masks = create_attn_masks(input_ids)\n",
        "      input_ids = torch.from_numpy(input_ids)\n",
        "      attention_masks = torch.tensor(attention_masks)\n",
        "      labels = torch.tensor(labels)\n",
        "    \n",
        "      test_sentences = test.text_cleaned.values\n",
        "      test_labels = test.tag.values\n",
        "            \n",
        "      # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "      \n",
        "      test_input_ids = tokenize_inputs(test_sentences, tokenizer, num_embeddings=MAX_LEN)\n",
        "      test_attention_masks = create_attn_masks(test_input_ids)\n",
        "      test_input_ids = torch.from_numpy(test_input_ids)\n",
        "      test_attention_masks = torch.tensor(test_attention_masks)\n",
        "      test_labels = torch.tensor(test_labels)\n",
        "\n",
        "     \n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "      \n",
        "      # Combine the training inputs into a TensorDataset.\n",
        "      train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "      \n",
        "      val_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "      \n",
        "      # Divide the dataset by randomly selecting samples.\n",
        "      #train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "      \n",
        "      # Checking whether the distribution of target is consitent across both the sets\n",
        "      label_temp_list = []\n",
        "      for a,b,c in train_dataset:\n",
        "        label_temp_list.append(c)\n",
        "      \n",
        "     \n",
        "      print('{:>5,} training samples with stress'.format(sum(label_temp_list)))\n",
        "      \n",
        "      label_temp_list = []\n",
        "      for a,b,c in val_dataset:\n",
        "        label_temp_list.append(c)\n",
        "      \n",
        "      \n",
        "      print('{:>5,} validation samples with stress'.format(sum(label_temp_list)))\n",
        "      \n",
        "      \n",
        "      # Create the DataLoaders for our training and validation sets.\n",
        "      # We'll take training samples in random order. \n",
        "      train_dataloader = DataLoader(\n",
        "                  train_dataset,  # The training samples.\n",
        "                  sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                  batch_size = batch_size # Trains with this batch size.\n",
        "              )\n",
        "      \n",
        "      # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "      validation_dataloader = DataLoader(\n",
        "                  val_dataset, # The validation samples.\n",
        "                  sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                  batch_size = batch_size # Evaluate with this batch size.\n",
        "              )\n",
        "      model = XLNetForMultiLabelSequenceClassification(num_labels=len(labels.unique()))\n",
        "      param_optimizer = list(model.named_parameters())\n",
        "      no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "      optimizer_grouped_parameters = [\n",
        "                                      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
        "      ]\n",
        "      optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
        "      \n",
        "      torch.cuda.empty_cache()\n",
        "      num_epochs = 10\n",
        "      \n",
        "      cwd = os.getcwd()\n",
        "      \n",
        "      model_save_path = output_model_file = \"/xlnet.bin\" \n",
        "      \n",
        "      \n",
        "      model, train_loss_set, valid_loss_set, training_stats = train_def(model=model,\\\n",
        "                                                                    num_epochs=num_epochs,\\\n",
        "                                                                    optimizer=optimizer,\\\n",
        "                                                                    train_dataloader=train_dataloader,\\\n",
        "                                                                    valid_dataloader=validation_dataloader,\\\n",
        "                                                                    model_save_path=model_save_path,\\\n",
        "                                                                    device=\"cuda\"#\"cuda\"\n",
        "                                                                    )\n",
        "      \n",
        "    save_path = \"/xlnet.pth\"\n",
        "      \n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "      #model = TheModelClass(*args, **kwargs)\n",
        "    model = XLNetForMultiLabelSequenceClassification(num_labels=len(labels.unique()))\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    model.to(device)\n",
        "    test = pd.read_csv(r'../input/stress-analysis-in-social-media/dreaddit-test.csv')\n",
        "    test_x = test.text.tolist()\n",
        "    test_y = test.label.tolist()\n",
        "    data = list(zip(test_x, test_y))\n",
        "    test = pd.DataFrame(data_test, columns=['text','tag'])\n",
        "\n",
        "    test['text_cleaned'] = list(map(lambda x:  preprocess_sentence_english(x),test['text']) )\n",
        "\n",
        "    # Get the lists of sentences and their labels\n",
        "    sentences = test.text_cleaned.values\n",
        "\n",
        "      # input_ids = torch.from_numpy(input_ids)\n",
        "      # attention_masks = torch.tensor(attention_masks)\n",
        "      # labels = torch.tensor(labels)\n",
        "\n",
        "    test_input_ids = tokenize_inputs(sentences, tokenizer, num_embeddings=MAX_LEN)\n",
        "    test_attention_masks = create_attn_masks(test_input_ids)\n",
        "\n",
        "    test[\"features\"] = test_input_ids.tolist()\n",
        "    test[\"masks\"] = test_attention_masks\n",
        "    pred_probs = generate_predictions(model, test, device=\"cuda\", batch_size=4)\n",
        "      # pred_probs\n",
        "\n",
        "    statistics.mean(pred_probs)\n",
        "    test['target'] = pred_probs\n",
        "    test['target'] = np.array(test['target'] >= 0.5, dtype='int')\n",
        "\n",
        "    auc_value = roc_auc_score(test['tag'], np.asarray(pred_probs) >0.5)\n",
        "    print(\"auc  on test {}\".format(auc_value))\n",
        "    prec, rec, f1 = Evaluate(test['tag'], np.asarray(pred_probs) >0.5) \n",
        "    print(prec)\n",
        "    print(rec)\n",
        "    print(f1)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reference - http://restanalytics.com/2021-05-04-Fine-Tuning-XLNet-For-Sequence-Classification/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('jumio')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "95c621ecdb3e798df7bac9633ecd8035868b63b0d3de9e7e73319b5117e62c2d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
